{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCuCHVPZDUD9A4DNWr8Hgt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amkutuzova/clinic/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9uYtO9tT3TFe"
      },
      "outputs": [],
      "source": [
        "pip install yake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1VcPrmqIQob"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizerFast\n",
        "from nltk.corpus import stopwords\n",
        "from pymystem3 import Mystem\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import nltk\n",
        "import string\n",
        "import yake\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "\n",
        "#предобработка текста, нормализуем текст\n",
        "def normalize_text(file_name):\n",
        "    mystem = Mystem()\n",
        "    russian_stopwords = stopwords.words(\"russian\")\n",
        "    punctuation = string.punctuation #знаки препинания\n",
        "\n",
        "    tokens = mystem.lemmatize(file_name.lower()) #нижний регистр\n",
        "\n",
        "    clean_tokens = [] #токенизация и стоп-слова\n",
        "    for token in tokens:\n",
        "        if token not in russian_stopwords and token.isalnum():\n",
        "            clean_tokens.append(token)\n",
        "    text = \" \".join(clean_tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "#ключевые слова и оценка уверенности модели для каждого предобработанного отзыва\n",
        "def find_keywords(file_name):\n",
        "    kw_extractor = yake.KeywordExtractor() #инициализация экстрактора yake\n",
        "    keywords = kw_extractor.extract_keywords(file_name)\n",
        "    return [(kw[0], float(kw[1])) for kw in keywords]\n",
        "\n",
        "\n",
        "#загрузка модели для анализа тональности\n",
        "tokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment', return_dict=True)\n",
        "\n",
        "#анализ тональности с помощью предобученной модели\n",
        "def analyze_sentiment(file_name):\n",
        "    inputs = tokenizer(file_name, padding=True, truncation=True, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    predicted = torch.argmax(predicted, dim=1).item()\n",
        "    return predicted\n",
        "\n",
        "df = pd.read_excel('/content/Исходные данные. ЦК.xlsx')\n",
        "\n",
        "#заменяем пустые строковые значения\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':  #только для строковых столбцов\n",
        "        df[col] = df[col].fillna('н/о')\n",
        "\n",
        "\n",
        "df['normalized_text'] = df['Тело'].apply(normalize_text) #нормализация\n",
        "df['key_words'] = df['normalized_text'].apply(find_keywords) #извлечение ключей\n",
        "df['sentiment'] = df['Тело'].apply(analyze_sentiment) #анализ тональности\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 6)) #визуализация результатов\n",
        "\n",
        "\n",
        "#график 1. Распределение тональности\n",
        "#преобразование тональности в категориальные метки\n",
        "plt.subplot(1, 2, 1)    #1 строка, 2 столбца, 1я позиция\n",
        "sentiment_labels = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "df['sentiment_label'] = df['sentiment'].map(sentiment_labels)\n",
        "sentiment_counts = df['sentiment_label'].value_counts()\n",
        "\n",
        "#сопоставление цветов по категориям\n",
        "colors = []\n",
        "for label in sentiment_counts.index:\n",
        "    if label == \"Negative\":\n",
        "        colors.append('red')\n",
        "    if label == \"Neutral\":\n",
        "        colors.append('gray')\n",
        "    if label == \"Positive\":\n",
        "        colors.append('green')\n",
        "\n",
        "plt.bar(sentiment_counts.index, sentiment_counts.values, color=colors)\n",
        "plt.xlabel('Тональность')\n",
        "plt.ylabel('Количество отзывов')\n",
        "plt.title('Распределение тональности отзывов')\n",
        "\n",
        "\n",
        "#график 2. Распределение отзывов по полу и тональности\n",
        "plt.subplot(1, 2, 2)  #1 строка, 2 столбца, 2я позиция\n",
        "gender_sentiment = df.groupby(['Пол', 'sentiment_label']).size().unstack() #группируем по полу и тональности\n",
        "gender_sentiment = gender_sentiment[['Negative', 'Neutral', 'Positive']]   #упорядочиваем столбцы\n",
        "\n",
        "gender_sentiment.plot(kind='bar', color=['red', 'gray', 'green'], ax=plt.gca()) #диаграмма с накоплением\n",
        "\n",
        "plt.xlabel('Пол')\n",
        "plt.ylabel('Количество отзывов')\n",
        "plt.title('Распределение отзывов по полу и тональности')\n",
        "plt.legend(title='Тональность:')\n",
        "\n",
        "\n",
        "#график 3. Топ10 ключевых слов\n",
        "#собираем в 1 список все ключевые слова\n",
        "all_keywords = []\n",
        "for keyword_list in df['key_words']:\n",
        "    for keyword, score in keyword_list:\n",
        "        all_keywords.append(keyword.lower())  #нижний регистр\n",
        "\n",
        "#считаем частоту слов со словарем\n",
        "word_counts = {}\n",
        "for word in all_keywords:\n",
        "    if word in word_counts:\n",
        "        word_counts[word] += 1\n",
        "    else:\n",
        "        word_counts[word] = 1\n",
        "\n",
        "#сортируем по частоте\n",
        "sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "top_words = sorted_words[:10]\n",
        "\n",
        "#создаем данные для гарфика\n",
        "words = [word for word, count in top_words]\n",
        "counts = [count for word, count in top_words]\n",
        "\n",
        "#горизонтальная столбчатая диаграмма\n",
        "plt.figure(figsize=(12, 8))\n",
        "y_pos = np.arange(len(words))\n",
        "\n",
        "plt.barh(y_pos, counts, align='center', color='blue')\n",
        "plt.yticks(y_pos, words)\n",
        "plt.xlabel('Частота использования')\n",
        "plt.title('Топ-10 частых ключевых слов в отзывах')\n",
        "plt.gca().invert_yaxis()  #по частоте сверху вниз\n",
        "\n",
        "#значения на столбцы\n",
        "for i, n in enumerate(counts):\n",
        "    plt.text(n + 0.5, i, str(n), color='black', va='center')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#сохраняем результаты\n",
        "df.to_excel('Обработанные_отзывы.xlsx')\n",
        "print(\"Результаты сохранены в 'Обработанные_отзывы.xlsx'\")\n",
        "\n",
        "\n",
        "normalize_text('/content/Исходные данные. ЦК.xlsx')\n",
        "find_keywords('/content/Исходные данные. ЦК.xlsx')\n",
        "analyze_sentiment('/content/Исходные данные. ЦК.xlsx')"
      ]
    }
  ]
}